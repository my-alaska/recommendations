{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - recommendations for news services\n",
    "\n",
    "## Preparation\n",
    "\n",
    " * download and unpack the dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * read more here: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [optinal] create a python virtual enviroment\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * install needed libraries:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "# import needed packages and libraries\n",
    "\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Chance to improve the performance time with knn algorithms\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining constants\n",
    "\n",
    "PATH = './MINDsmall_train'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "# loading text metadata\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text normalization\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # removing punctuation and digits. Converting to lowercase\n",
    "    text = text.translate(str.maketrans(\"\", \"\", punctuation+\"0123456789\")).lower()   \n",
    "    # tokenization\n",
    "    text = text.split()\n",
    "    # removing stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "def stem_texts(corpus):\n",
    "#     stemmer = RSLPStemmer()\n",
    "#     stemmer = WordNetLemmatizer.()\n",
    "    stemmer = LancasterStemmer()\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stemmer = SnowballStemmer()\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "think real good team team real spec good thing group clos bri schmetzer\n"
     ]
    }
   ],
   "source": [
    "# Let's compare some text before and after processing\n",
    "\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12304\\751681367.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_words_sorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mword_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12304\\751681367.py\u001b[0m in \u001b[0;36mget_all_words_sorted\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_all_words_sorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_words_sorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# create a list of all words in the corpus\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    return sorted([word for word in text for text in corpus])\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of texts in which each of the words appeard\n",
    "# If it appeared multiple times in one text we still count it only once\n",
    "\n",
    "def get_document_frequencies(corpus, wordlist):\n",
    "    # return {word -> count}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "# calculate the number of word occurrences in each text\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    # return {news_id -> {word -> count}}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the tf_idf metric\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between 2 elements/texts\n",
    "# test different metrics and chose one\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[2], news_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for a given text find k most similar ones\n",
    "# remember about the proper sorting order for used metric\n",
    "# remember to not include the currently checked text. \n",
    "# It would always be the most similar because it's identical\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus, news_indices):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k)\n",
    "    print(f'id: {n_id}, text: {corpus[news_indices[n_id]]}')\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "        print(f'\\nid: {s_id}, text: {corpus[news_indices[s_id]]}')\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[42337], 5, texts, news_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
